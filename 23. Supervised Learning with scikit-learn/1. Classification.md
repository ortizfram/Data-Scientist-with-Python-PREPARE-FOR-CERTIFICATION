# Binary classification
**In the video, you saw that there are two types of supervised learning — classification and regression. Recall that binary classification is used to predict a target variable that has only two labels, typically represented numerically with a zero or a one.**

A dataset, churn_df, has been preloaded for you in the console.
- Your task is to examine the data and choose which column could be the target variable for binary classification.

      In [2]:
      churn_df.head()
- [x] "churn"
> churn has values of `0 or 1`, so it can be predicted using a `binary classification model.`
# The supervised learning workflow
      from sklearn.module import Model
      model = Model()
      model.fit(X, y)
      model.predict(X_new)
# k-Nearest Neighbors: Fit
```py
# Import KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier 

# Create arrays for the features and the target variable
y = churn_df["churn"].values
X = churn_df[["customer_service_calls", "account_length"]].values

# Create a KNN classifier with 6 neighbors
knn = KNeighborsClassifier(n_neighbors=6)

# Fit the classifier to the data
knn.fit(X, y)
```
# k-Nearest Neighbors: Predict
      X_new = np.array([[30.0, 17.5],
                  [107.0, 24.1],
                  [213.0, 10.9]])
```py
# Predict the labels for the X_new
y_pred = knn.predict(X_new) #nkk = model

# Print the predictions for X_new
print("Predictions: {}".format(y_pred)) 

# Predictions: [0 1 0]
```
> Great work! The model has predicted the first and third customers will not churn in the new array. But how do we know how accurate these predictions are?

# Accuracy formula
      correct predictions / total observations
### steps for Computing Accuracy
<img src="https://user-images.githubusercontent.com/51888893/212555714-6fa77413-c884-452a-9b32-1c4f9dae574f.png" width=900px>

### train/test split
      from sklearn.model_selection import train_test_split
      
      X_train, X_test, y_train, y_test = train_test_split(X, Y, text_size=0.3,
                                                          random_state=21, stratify=y)
      knn = KNeighborsClassifier(n_neighbors=6)
      knn.fit(X_train, y_train)
      print(knn.score(X_test, y_test))
      
      # 0.8800599700149925
# Train/test split + computing accuracy
```py
# Import the module
from sklearn.model_selection import train_test_split

X = churn_df.drop("churn", axis=1).values
y = churn_df["churn"].values

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
                                                    random_state=42, stratify=y)
knn = KNeighborsClassifier(n_neighbors=5)

# Fit the classifier to the training data
knn.fit(X_train, y_train)

# Print the accuracy
print(knn.score(X_test, y_test)) 

 # accuracy : 0.8740629685157422
```
# Overfitting and underfitting
Interpreting model complexity is a great way to evaluate performance when utilizing supervised learning. Your aim is to produce a model that can interpret the relationship between features and the target variable, as well as generalize well when exposed to new observations.
```py
# Create neighbors
neighbors = np.arange(1, 13)
train_accuracies = {}
test_accuracies = {}

for neighbor in neighbors:

	# Set up a KNN Classifier
	knn = KNeighborsClassifier(n_neighbors = neighbor)
  
	# Fit the model
	knn.fit(X_train, y_train)
  
	# Compute accuracy
	train_accuracies[neighbor] = knn.score(X_train, y_train)
	test_accuracies[neighbor] = knn.score(X_test, y_test)
print(neighbors, '\n', train_accuracies, '\n', test_accuracies)
```
# Visualizing model complexity
```py
# Add a title
plt.title("KNN: Varying Number of Neighbors")

# Plot training accuracies
plt.plot(neighbors, train_accuracies.values(), label="Training Accuracy")

# Plot test accuracies
plt.plot(neighbors, test_accuracies.values(), label="Testing Accuracy")

plt.legend()
plt.xlabel("Number of Neighbors")
plt.ylabel("Accuracy")

# Display the plot
plt.show()
```
<img src="https://user-images.githubusercontent.com/51888893/212666553-fab5f0a6-ef93-42f8-959a-f90065b1f552.png" wifth=400px>
