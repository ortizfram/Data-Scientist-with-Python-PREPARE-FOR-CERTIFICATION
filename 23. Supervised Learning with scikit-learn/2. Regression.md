# Creating features
```py
import numpy as np

# Create X from the radio column's values
X = sales_df['radio'].values

# Create y from the sales column's values
y = sales_df['sales'].values

# Reshape X
X = X.reshape(-1, 1)

# Check the shape of the features and targets
print(X.shape, y.shape)

# (4546, 1) (4546,)
```
# Building a linear regression model
```py
# Import LinearRegression
from sklearn.linear_model import LinearRegression

# Create the model
reg = LinearRegression()

# Fit the model to the data
reg.fit(X, y)

# Make predictions
predictions = reg.predict(X)

print(predictions[:5])
```
# Visualizing a linear regression model
```py
# Import matplotlib.pyplot
import matplotlib.pyplot as plt

# Create scatter plot
# X: radio values, y: array of sales values
plt.scatter(X, y, color="blue")

# Create line plot
# predictions: an array of the model's predicted values for 'y' given 'X'
plt.plot(X, predictions, color="red")
plt.xlabel("Radio Expenditure ($)")
plt.ylabel("Sales ($)")

# Display the plot
plt.show()
```
<img src="https://user-images.githubusercontent.com/51888893/212672525-0a559d1e-7dea-46f0-8670-f08f21816cc5.png" width=400px>

# Fit and predict for regression
```py
# Create X and y arrays
X = sales_df.drop("sales", axis=1).values
y = sales_df["sales"].values

X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.3, random_state=42)

# Instantiate the model
reg = LinearRegression()

# Fit the model to the data
reg.fit(X_train, y_train)

# Make predictions
y_pred = reg.predict(X_test)
print("Predictions: {}, Actual Values: {}".format(y_pred[:2], y_test[:2]))
```
# Regression performance
```py
# Import mean_squared_error
from sklearn.metrics import mean_squared_error

# Compute R-squared
r_squared = reg.score(X_test, y_test)

# Compute RMSE
rmse = mean_squared_error(y_test, y_pred, squared=False)

# Print the metrics
print("R^2: {}".format(r_squared))
print("RMSE: {}".format(rmse))

# R^2: 0.9990165886162027
# RMSE: 2942.372219812037

'''features explain 99.9% of the variance in sales values! Looks like this company's advertising strategy is working well!'''
```
# Cross-validation for R-squared
```py
# Import the necessary modules
from sklearn.model_selection import cross_val_score, KFold

#Â Create a KFold object
kf = KFold(n_splits=6, shuffle=True, random_state=5)

reg = LinearRegression()

# Compute 6-fold cross-validation scores
cv_scores = cross_val_score(reg, X, y, cv=kf)

# Print scores
print(cv_scores)

# [0.74451678 0.77241887 0.76842114 0.7410406  0.75170022 0.74406484]

'''R-squared for each fold ranged between 0.74 and 0.77? By using cross-validation, we can see how performance varies depending on how the data is split!'''
```
# Analyzing cross-validation metrics
```py
# Print the mean
print(np.mean(cv_results))

# Print the standard deviation
print(np.std(cv_results))

# Print the 95% confidence interval
print(np.quantile(cv_results, [0.25, 0.95]))


# Print the mean
print(np.mean(cv_results))

# Print the standard deviation
print(np.std(cv_results))

# Print the 95% confidence interval
print(np.quantile(cv_results, [0.025, 0.975]))


# 0.7536937416666666
# 0.012305386274436092
# [0.74141863 0.77191915]

'''average score of 0.75 with a low standard deviation is pretty good for a mode'''
```
