# ==> Comparing strings
## -- Minimum edit distance
- least amount of steps to transform one string into an other by...
  - `insertion`
  - `deletion`
  - `substitution`
  - `transposition`
    - package : `thefuzz`  
```py 
from thefuzz import fuzz
# compare reading vs reeding
fuzz.Wratio('Reeding', 'Reading') #86
'''from 0 to 100 similarity'''

# compare string similarity
# Define string and array of possible matches
string = "Houston Rockets vs Los Angeles Lakers"
choices = pd.Series(['Rockets vs Lakers', 'Lakers vs Rockets',
                    'Houson vs Los Angeles', 'Heat vs Bulls '])
process.extract(string, choices, Limit = 2)
```
# Minimum edit distance
> What is the minimum edit distance from 'sign' to 'sing', and which operation(s) gets you there?
- [ ] 2 by substituting 'g' with 'n' and 'n' with 'g'.
- [x] 1 by transposing 'g' with 'n'.
- [ ] 1 by substituting 'g' with 'n'.
- [ ] 2 by deleting 'g' and inserting a new 'g' at the end.
# The cutoff point
- Import process from thefuzz.
- Store the unique cuisine_types into unique_types.
- Calculate the similarity of 'asian', 'american', and 'italian' to all possible cuisine_types using process.extract(), while returning all possible matches.
```py
# Import process from thefuzz
from thefuzz import process

# Store the unique values of cuisine_type in unique_types
unique_types = restaurants['cuisine_type'].unique()

# Calculate similarity of 'asian' to all values of unique_types
print(process.extract('asian', unique_types, limit = len(unique_types)))

# Calculate similarity of 'american' to all values of unique_types
print(process.extract('american', unique_types, limit = len(unique_types)))

# Calculate similarity of 'italian' to all values of unique_types
print(process.extract('italian', unique_types, limit = len(unique_types)))
```
- Question
> Take a look at the output, what do you think should be the similarity cutoff point when remapping categories?

Possible Answers
- [x] 80
- [ ] 70
- [ ] 60
# Remapping categories II
- Return all of the unique values in the cuisine_type column of restaurants.
```py
# Inspect the unique values of the cuisine_type column
print(restaurants['cuisine_type'].unique())
```
- As a first step, create a list of all possible matches, comparing 'italian' with the restaurant types listed in the cuisine_type column.
```py
# Create a list of matches, comparing 'italian' with the cuisine_type column
matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))

# Inspect the first 5 matches
print(matches[0:5])
```
- Within the for loop, use an if statement to check whether the similarity score in each match is greater than or equal to 80.
- If it is, use .loc to select rows where cuisine_type in restaurants is equal to the current match (which is the first element of match), and reassign them to be 'italian'.
```py
# Create a list of matches, comparing 'italian' with the cuisine_type column
matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants))

# Iterate through the list of matches to italian
for match in matches:
  # Check whether the similarity score is greater than or equal to 80
  if match[1]>=80:
    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine
    restaurants.loc[restaurants['cuisine_type'] == match[0], 'cuisine_type'] = 'italian'
```
Finally, you'll adapt your code to work with every restaurant type in categories.
- Using the variable cuisine to iterate through categories, embed your code from the previous step in an outer for loop.
- Inspect the final result. This has been done for you.
```py
# Iterate through categories
for cuisine in categories:  
  # Create a list of matches, comparing cuisine with the cuisine_type column
  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))

  # Iterate through the list of matches
  for match in matches:
     # Check whether the similarity score is greater than or equal to 80
    if match[1] >= 80:
      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine
      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine
      
# Inspect the final result
print(restaurants['cuisine_type'].unique())
```
# ==> Generating pairs
```py
import recordlinkage

# create indexing object
indexer = recordlinkage.Index()
# Gnerate pairs blocked in state
Indexer.block('state')
pairs =  Indexer.index(census_A, census_B)

# Create compare Object
compare_cl = recordlinkage.Compare()
# Find exact matches for pairs of date_of-birth and state
compare_cl.exact (' date_of_birth', 'date_of_birth', Label= 'date_of_birth ')
compare_cl.exact (' state', 'state', label= 'state')

# Find similar matches for pairs of surname and address_1 using string similarity
compare_cl.string (' surname", surname", threshold=0.85, label='surname')
compare cl.string (" address_1 ', 'address_1', threshold=0. 85, abel= 'address_1')

#Find matches
potential_matches = compare_cl.compute (pairs, census_A, census_B)
```
### > finding only pairs we want
```py
# 1 is match 0 isn't. Here que want more than 1 col matched 
potential_matches [potential_matches.sum(axis = 1) => 2]
```
# To link or not to link?
- Record linkage

      Using an address column to join two DataFrames, with the address
      in each DataFrame being formatted slightly differently.
      Merging two basketball DataFrames, with columns team_A
      teamB,and time and differently formatted team names between
      each DataFrame.
      Two customer DataFrames containing names and address, one with
      a unique identifier per customer, one without.
- Regular joins

      Two basketball DataFrames with a common unique identifier per
      game
      Consolidating two DataFrames containing details on DataCamp
      courses, with each DataCamp course having its own unique
      identifier.
# Pairs of restaurants
1
- Instantiate an indexing object by using the Index() function from recordlinkage.
- Block your pairing on cuisine_type by using indexer's' .block() method.
- Generate pairs by indexing restaurants and restaurants_new in that order.
```py
# Create an indexer and object and find possible pairs
indexer = recordlinkage.Index()

# Block pairing on cuisine_type
indexer.block('cuisine_type')

# Generate pairs
pairs = indexer.index(restaurants, restaurants_new)
```
- Question
Now that you've generated your pairs, you've achieved the first step of record linkage. What are the steps remaining to link both restaurants DataFrames, and in what order?

Possible Answers
- [x] Compare between columns, score the comparison, then link the DataFrames.
- [ ] Clean the data, compare between columns, link the DataFrames, then score the comparison.
- [ ] Clean the data, compare between columns, score the comparison, then link the DataFrames.
# Similar restaurants
1
- Instantiate a comparison object using the recordlinkage.Compare() function.
```py
# Create a comparison object
comp_cl = recordlinkage.Compare()
```
2
- Use the appropriate comp_cl method to find exact matches between the city and cuisine_type columns of both DataFrames.
- Use the appropriate comp_cl method to find similar strings with a 0.8 similarity threshold in the rest_name column of both DataFrames.
```py
# Create a comparison object
comp_cl = recordlinkage.Compare()

# Find exact matches on city, cuisine_types 
comp_cl.exact('city', 'city', label='city')
comp_cl.exact('cuisine_type', 'cuisine_type', label = 'cuisine_type')

# Find similar matches of rest_name
comp_cl.string('rest_name', 'rest_name', threshold=0.8, label = 'rest_name') 
```
3
- Compute the comparison of the pairs by using the .compute() method of comp_cl.
```py
# Create a comparison object
comp_cl = recordlinkage.Compare()

# Find exact matches on city, cuisine_types - 
comp_cl.exact('city', 'city', label='city')
comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')

# Find similar matches of rest_name
comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) 

# Get potential matches and print
potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)
print(potential_matches)
```
4
- Question
Print out potential_matches, the columns are the columns being compared, with values being 1 for a match, and 0 for not a match for each pair of rows in your DataFrames. To find potential matches, you need to find rows with more than matching value in a column. You can find them with

      potential_matches[potential_matches.sum(axis = 1) >= n]
    
Where n is the minimum number of columns you want matching to ensure a proper duplicate find, what do you think should the value of n be?

Possible Answers
- [x] 3 because I need to have matches in all my columns.
- [ ] 2 because matching on any of the 2 columns or more is enough to find potential duplicates.
- [ ] 1 because matching on just 1 column like the restaurant name is enough to find potential duplicates.
# ==> Linking DataFrames
# Getting the right index
Here's a DataFrame named matches containing potential matches between two DataFrames, users_1 and users_2. Each DataFrame's row indices is stored in uid_1 and uid_2 respectively.

                 first_name  address_1  address_2  marriage_status  date_of_birth
    uid_1 uid_2                                                                  
    0     3              1          1          1                1              0
         ...            ...         ...        ...              ...            ...
         ...            ...         ...        ...              ...            ...
    1     3              1          1          1                1              0
         ...            ...         ...        ...              ...            ...
         ...            ...         ...        ...              ...            ...
How do you extract all values of the uid_1 index column?
- [ ] matches.index.get_level_values(0)
- [ ] matches.index.get_level_values(1)
- [ ] matches.index.get_level_values('uid_1')
- [x] Both 1 and 3 are correct.
# Linking them together!
- Isolate instances of potential_matches where the row sum is above or equal to 3 by using the .sum() method.
- Extract the second column index from matches, which represents row indices of matching record from restaurants_new by using the .get_level_values() method.
- Subset restaurants_new for rows that are not in matching_indices.
- Append non_dup to restaurants.
```py
# Isolate potential matches with row sum >=3
matches = potential_matches[potential_matches.sum(axis=1) >= 3]

# Get values of second column index of matches
matching_indices = matches.index.get_level_values(1)

# Subset restaurants_new based on non-duplicate values
non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]

# Append non_dup to restaurants
full_restaurants = restaurants.append(non_dup)
print(full_restaurants)
```


