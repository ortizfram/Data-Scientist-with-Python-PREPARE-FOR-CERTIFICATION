# ==> Data type constraints
# -- Data Science Workflow
<img src="https://user-images.githubusercontent.com/51888893/210350830-d7c27b38-59e4-4e7a-b9ad-2a90bd45e954.png" width=400px>

If we don't clean we'll compromise insights and reports generated
### > see data types
      sales.dtypes
      # if it's 'object' means it's a String
### > see data info + missing values
      sales.info()
### > see data column statistics summary
      sales['user_type'].describe()
## --> string to int
```py
# summing this will give a large string instead of the total revenue cause it's an object (string)
sales['Revenue'].sum
# '2342234$8868271$9192839$'

# Remove the '$' from Revenue column
sales['Revenue'] = sales['Revenue'].str.strip('$') # remove '$'
sales['Revenue'] = sales['Revenue'].astype('int') # to int

# Assert method for checking dtype
assert sales['Revenue'].dtype == 'int' # if it's met returns nothing, error if it's not
```
# Common data types
- Numeric data types

      Number of points on customer loyalty
      card
      Number of items bought in a basket
      Salary earned monthly
- Text

      City of residence
      Shipping address of a customer
      First name
- Dates

      Order date of a product
      Birthdates of clients
# Numeric data or ... ?
- Print the information of ride_sharing.
- Use .describe() to print the summary statistics of the user_type column from ride_sharing.
```py
# Print the information of ride_sharing
print(ride_sharing.info())

# Print summary statistics of user_type column
print(ride_sharing['user_type'].describe())
```
Question
> By looking at the summary statistics - they don't really seem to offer much description on how users are distributed along their purchase type, why do you think that is?
- Possible Answers
- [ ] The user_type column is not of the correct type, it should be converted to str.
- [ ] The user_type column has an infinite set of possible values, it should be converted to category.
- [x] The user_type column has an finite set of possible values that represent groupings of data, it should be converted to category
---
- Convert user_type into categorical by assigning it the 'category' data type and store it in the user_type_cat column.
- Make sure you converted user_type_cat correctly by using an assert statement.
```py
# Print the information of ride_sharing
print(ride_sharing.info())

# Print summary statistics of user_type column
print(ride_sharing['user_type'].describe())

# Convert user_type from integer to category
ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')

# Write an assert statement confirming the change
assert ride_sharing['user_type_cat'].dtype == 'category'

# Print new summary statistics 
print(ride_sharing['user_type_cat'].describe()) # described as category to see distribution
```
# Summing strings and concatenating numbers
- Use the .strip() method to strip duration of "minutes" and store it in the duration_trim column.
- Convert duration_trim to int and store it in the duration_time column.
- Write an assert statement that checks if duration_time's data type is now an int.
- Print the average ride duration.
```py
# Strip duration of minutes
'''remove 'minutes' str in a new col'''
ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip("minutes") 

# Convert duration to integer
'''new col passing to duration to int'''
ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int') 

# Write an assert statement making sure of conversion
assert ride_sharing['duration_time'].dtype == 'int'

# Print formed columns and calculate average ride duration 
print(ride_sharing[['duration','duration_trim','duration_time']])
print(ride_sharing['duration_time'].mean())
```
      11.389052795031056
# ==> Data range constraints
### > todays data
```py
import datetime as dt
today_data = dt.date.today()
```
## --> dropping values
```py
# drop values by filtering
movies = movies[movies['avg_rating']>5]

# drop values using .drop() method
movies.drop(movies[movies['avg_rating']>5].index, inplace = True) # inplace to not create a new col

# Assert results
assert movies['avg_rating'].max() <= 5

# Or replace higher than 5 == 5
movies.loc[movies['avg_rating'] > 5, 'avg_rating'] = 5
```
### > convert str to date
```py
user_signups['subscription_date'] = pd.to_datetime(user_signups['subscription_date']).dt.date
```
# Tire size constraints
- Convert the tire_sizes column from category to 'int'.
- Use .loc[] to set all values of tire_sizes above 27 to 27.
- Reconvert back tire_sizes to 'category' from int.
- Print the description of the tire_sizes.
```py
# Convert tire_sizes to integer
ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')

# Set all values above 27 to 27
ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27 # convert to int to replace larger ones

# Reconvert tire_sizes back to categorical
ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category') # back to category to see distribution

# Print tire size description
print(ride_sharing['tire_sizes'].describe())
```
      count     25760
      unique        2
      top          27 # max num
      freq      13274
      Name: tire_sizes, dtype: int64
# Back to the future
- Convert ride_date to a datetime object using to_datetime(), then convert the datetime object into a date and store it in ride_dt column.
- Create the variable today, which stores today's date by using the dt.date.today() function.
- For all instances of ride_dt in the future, set them to today's date.
- Print the maximum date in the ride_dt column.
```py
# Convert ride_date to date
ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date # easy way. datetime object into a date

# Save today's date
today = dt.date.today()

# Set all in the future to today's date
ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today

# Print maximum of ride_dt column
print(ride_sharing['ride_dt'].max())
```
      2023-01-03
# ==> Uniqueness constraints
### > find duplicated
```py
# boolean
duplicates = height_weight.duplicated()
print(duplicates)

# Return duplicates
duplicates = height_weight.duplicated()
height_weight[duplicates]
```
## -- duplicated() method
- `subset` : columns to check if duplicated
- `keep` : whether to keep ***'first','last', 'False'(all)***
- `inplace`  : do not create other column when dropping (False)
### > treat duplciates statistical summaries
```py
# group by cols and produce statistical summaries
column_names = ['first_name', 'last_name', 'address']
summaries = {'height' : 'max', 'weight': 'mean'} # this for .agg method
height_weight = height_weight.groupby(by= column_names).agg(summaries).reset_index() 
```
# How big is your subset?
> Choose the correct usage of .duplicated() below:
- [ ] loans.duplicated()
  Because the default method returns both complete and incomplete duplicates.
  
- [ ] loans.duplicated(subset = 'first_name')
  Because constraining the duplicate rows to the first name lets me find incomplete duplicates as well.
  
- [x] loans.duplicated(subset = ['first_name', 'last_name'], keep = False)
  Because subsetting on consumer metadata and not discarding any duplicate returns all duplicated rows.
  
- [ ] loans.duplicated(subset = ['first_name', 'last_name'], keep = 'first')
  Because this drops all duplicates.
# Finding duplicates
- Find duplicated rows of ride_id in the ride_sharing DataFrame while setting keep to False.
- Subset ride_sharing on duplicates and sort by ride_id and assign the results to duplicated_rides.
- Print the ride_id, duration and user_birth_year columns of duplicated_rides in that order.
```py
# Find duplicates
duplicates = ride_sharing.duplicated(subset='ride_id', keep=False)

# Sort your duplicated rides
duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')

# Print relevant columns of duplicated_rides
print(duplicated_rides[['ride_id','duration','user_birth_year']])

''' id 33,89 incomplete duplicates'''
```
          ride_id  duration  user_birth_year
      22       33        10             1979
      39       33         2             1979
      53       55         9             1985
      65       55         9             1985
      74       71        11             1997
      75       71        11             1997
      76       89         9             1986
      77       89         9             2060
# Treating duplicates
- Drop complete duplicates in ride_sharing and store the results in ride_dup.
- Create the statistics dictionary which holds minimum aggregation for user_birth_year and mean aggregation for duration.
- Drop incomplete duplicates by grouping by ride_id and applying the aggregation in statistics.
- Find duplicates again and run the assert statement to verify de-duplication.
```py
# Drop complete duplicates from ride_sharing
ride_dup = ride_sharing.drop_duplicates()

# Create statistics dictionary for aggregation function
statistics = {'user_birth_year': 'min', 'duration': 'mean'}

# Group by ride_id and compute new statistics
ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()

# Find duplicated values again
duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)
duplicated_rides = ride_unique[duplicates == True]

# Assert duplicates are processed
assert duplicated_rides.shape[0] == 0
```
