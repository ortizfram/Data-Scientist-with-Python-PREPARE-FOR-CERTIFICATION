# ==> Uniformity
# Ambiguous dates
> You have a DataFrame containing a subscription_date column that was collected from various sources with different Date formats such as YYYY-mm-dd and YYYY-dd-mm. What is the best way to unify the formats for ambiguous values such as 2019-04-07?

Answer the question
50XP
Possible Answers
- [ ] Set them to NA and drop them.
- [ ] Infer the format of the data in question by checking the format of subsequent and previous values.
- [ ] Infer the format from the original data source.
- [x] All of the above are possible, as long as we investigate where our data comes from, and understand the dynamics affecting it before cleaning it.
# Uniform currencies
- Find the rows of acct_cur in banking that are equal to 'euro' and store them in the variable acct_eu.
- Find all the rows of acct_amount in banking that fit the acct_eu condition, and convert them to USD by multiplying them with 1.1.
- Find all the rows of acct_cur in banking that fit the acct_eu condition, set them to 'dollar'.
```py
'''You are tasked with understanding the average account size and how investments vary by the size of account,
however in order to produce this analysis accurately, you first need to unify the currency amount into dollars.'''

# Find values of acct_cur that are equal to 'euro'
acct_eu = banking['acct_cur'] == 'euro'

# Convert acct_amount where it is in euro to dollars
banking.loc[banking['acct_cur']=='euro', 'acct_amount'] = banking.loc[banking['acct_cur']=='euro', 'acct_amount'] * 1.1

# Unify acct_cur column by changing 'euro' values to 'dollar'
banking.loc[banking['acct_cur']=='euro', 'acct_cur'] = 'dollar'

# Assert that only dollar currency remains
assert banking['acct_cur'].unique() == 'dollar'
```
# Uniform dates
1
- Print the header of account_opened from the banking DataFrame and take a look at the different results.
```py
# Print the header of account_opened
print(banking['account_opened'].head())
```
---
2

Question
> Take a look at the output. You tried converting the values to datetime using the default to_datetime() function without changing any argument, however received the following error:

ValueError: month must be in 1..12
Why do you think that is?

- Possible Answers
- [ ] The to_datetime() function needs to be explicitly told which date format each row is in.
- [ ] The to_datetime() function can only be applied on YY-mm-dd date formats.
- [x] The 21-14-17 entry is erroneous and leads to an erro
---
3
- Convert the account_opened column to datetime, while making sure the date format is inferred and that erroneous formats that raise error return a missing value.
```py
# Print the header of account_opened
print(banking['account_opened'].head())

# Convert account_opened to datetime
banking['account_opened'] = pd.to_datetime(banking['account_opened'],
                                           # Infer datetime format
                                           infer_datetime_format = True,
                                           # Return missing value for error
                                           errors = 'coerce') 
```
---
4
- Extract the year from the amended account_opened column and assign it to the acct_year column.
- Print the newly created acct_year column.
```py
# Print the header of account_opend
print(banking['account_opened'].head())

# Convert account_opened to datetime
banking['account_opened'] = pd.to_datetime(banking['account_opened'],
                                           # Infer datetime format
                                           infer_datetime_format = True,
                                           # Return missing value for error
                                           errors = 'coerce') 

# Get year of account opened
banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')

# Print acct_year
print(banking['acct_year'])
```
# ==> Cross field validation
# Cross field or no cross field?
- Cross field validation

      Confirming the Age provided by users by cross checking their birthdays.
      Row wise operations such as .sum (axis = 1)
- Not cross field validation

      Making sure that a revenue column is a numeric column.
      Making sure a subscription_date column has no values set in the future.
# How's our data integrity?
- Find the rows where the sum of all rows of the fund_columns in banking are equal to the inv_amount column.
- Store the values of banking with consistent inv_amount in consistent_inv, and those with inconsistent ones in inconsistent_inv.
```py
# Store fund columns to sum against
fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']

# Find rows where fund_columns row sum == inv_amount
inv_equ = banking[fund_columns].sum(axis=1) == banking['inv_amount']

# Store consistent and inconsistent data
consistent_inv = banking[inv_equ]
inconsistent_inv = banking[~inv_equ]

# Store consistent and inconsistent data
print("Number of inconsistent investments: ", inconsistent_inv.shape[0])

'''Number of inconsistent investments:  8'''
```
- Store today's date into today, and manually calculate customers' ages and store them in ages_manual.
- Find all rows of banking where the age column is equal to ages_manual and then filter banking into consistent_ages and inconsistent_ages.
```py
# Store today's date and find ages
today = dt.date.today()
ages_manual = today.year - banking['birth_date'].dt.year

# Find rows where age column == ages_manual
age_equ = ages_manual == banking['age']

# Store consistent and inconsistent data
consistent_ages = banking[age_equ]
inconsistent_ages = banking[~age_equ]

# Store consistent and inconsistent data
print("Number of inconsistent ages: ", inconsistent_ages.shape[0])
'''Number of inconsistent ages:  4'''
```
# ==> Completeness
### > missingno package
 Useful package for visualizing and unders tanding missing data
```py
import missingno as msno
import matplotlib.pyplot as plt

# visualize missingness
msno.matrix(airquality)
plt.show()
```
<img src="https://user-images.githubusercontent.com/51888893/210449606-1e8614d4-963c-4c1d-9a89-e44ef5a8799c.png" width=400px>

```py
sorted_airquality= airquality.sort_values(by= 'temperature')
msno.matrix(sorted_airquality)
plt.show()
```
- missing values are on top (low temperatures)
<img src="https://user-images.githubusercontent.com/51888893/210449864-9258b8df-6670-4703-9326-a928cce72769.png" width=400px>

# Is this missing at random?
You've seen in the video exercise how there are a variety of missingness types when observing missing data. As a reminder, missingness types can be described as the following:

Missing Completely at Random: No systematic relationship between a column's missing values and other or own values.
Missing at Random: There is a systematic relationship between a column's missing values and other observed values.
Missing not at Random: There is a systematic relationship between a column's missing values and unobserved values.
You have a DataFrame containing customer satisfaction scores for a service. What type of missingness is the following?

> A customer satisfaction_score column with missing values for highly dissatisfied customers.

Answer the question
50XP
Possible Answers
- [ ] Missing completely at random.
- [ ] Missing at random.
- [x] Missing not at random.
# Missing investors
- Print the number of missing values by column in the banking DataFrame.
- Plot and show the missingness matrix of banking with the msno.matrix() function.
```py
# Print number of missing values in banking
print(banking['inv_amount'].isna().sum()) #13

# Visualize missingness matrix
msno.matrix(banking)
plt.show()
```
<img src="https://user-images.githubusercontent.com/51888893/210450648-5fa62b60-d347-4e69-be71-cfe2c67a7381.png" width=400px>

- Isolate the values of banking missing values of inv_amount into missing_investors and with non-missing inv_amount values into investors.
```py
# Print number of missing values in banking
print(banking.isna().sum())

# Visualize missingness matrix
msno.matrix(banking)
plt.show()

# Isolate missing and non missing values of inv_amount
missing_investors = banking[banking['inv_amount'].isna()]
investors = banking[~banking['inv_amount'].isna()]
```
---
Question
> Now that you've isolated banking into investors and missing_investors, use the .describe() method on both of these DataFrames in the IPython shell to understand whether there are structural differences between them. What do you think is going on?

Possible Answers
- [ ] The data is missing completely at random and there are no drivers behind the missingness.
- [x] The inv_amount is missing only for young customers, since the average age in missing_investors is 22 and the maximum age is 25.
- [ ] The inv_amount is missing only for old customers, since the average age in missing_investors is 42 and the maximum age is 59.
---
- Sort the banking DataFrame by the age column and plot the missingness matrix of banking_sorted.
```py
# Print number of missing values in banking
print(banking.isna().sum())

# Visualize missingness matrix
msno.matrix(banking)
plt.show()

# Isolate missing and non missing values of inv_amount
missing_investors = banking[banking['inv_amount'].isna()]
investors = banking[~banking['inv_amount'].isna()]

# Sort banking by age and visualize
banking_sorted = banking.sort_values(by= 'age')
msno.matrix(banking_sorted)
plt.show()
```
<img src="https://user-images.githubusercontent.com/51888893/210451647-02bc3d97-42c2-4016-95e3-5f60eddcdfef.png" width=400px>

# Follow the money
- Use .dropna() to drop missing values of the cust_id column in banking and store the results in banking_fullid.
- Use inv_amount to compute the estimated account amounts for banking_fullid by setting the amounts equal to inv_amount * 5, and assign the results to acct_imp.
- Impute the missing values of acct_amount in banking_fullid with the newly created acct_imp using .fillna().
```py
# Drop missing values of cust_id
banking_fullid = banking.dropna(subset = ['cust_id'])

# Compute estimated acct_amount
acct_imp = banking_fullid['inv_amount']*5

# Impute missing acct_amount with corresponding acct_imp
banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})

# Print number of missing values
print(banking_imputed.isna().sum())
```
