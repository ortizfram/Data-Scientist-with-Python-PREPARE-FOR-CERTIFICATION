# ==> Limits of simple regression
# Regression and causation
      
      Which of the following conclusions can we draw from this data?

      A. Eating a good diet leads to better health and higher income.
      B. People with higher income can afford a better diet.
      C. People with high income are more likely to be vegetarians.
      
      ANSW: NONE
# Using StatsModels
```py
from scipy.stats import linregress
import statsmodels.formula.api as smf

# Run regression with linregress
subset = brfss.dropna(subset=['INCOME2', '_VEGESU1'])
xs = subset['INCOME2']
ys = subset['_VEGESU1']
res = linregress(xs, ys)
print(res)

# Run regression with StatsModels
results = smf.ols('_VEGESU1 ~ INCOME2', data = brfss).fit()
print(results.params)
'''
LinregressResult(slope=0.06988048092105019, intercept=1.5287786243363106, rvalue=0.11967005884864099, pvalue=1.378503916248713e-238, stderr=0.002110976356332333, intercept_stderr=0.013196467544093607)
Intercept    1.529
INCOME2      0.070
dtype: float64
'''
```
# Plot income and education
```py
# Group by educ
grouped = gss.groupby('educ')

# Compute mean income in each group
mean_income_by_educ = grouped['realinc'].mean()

# Plot mean income as a scatter plot
plt.plot(mean_income_by_educ, 'o', alpha=0.5)

# Label the axes
plt.xlabel('Education (years)')
plt.ylabel('Income (1986 $)')
plt.show()

# relationship between income and education is non-linear.
```
<img src="https://user-images.githubusercontent.com/51888893/211322088-4394c520-d178-4e0c-9910-a1ee4d1b9d09.png" width=400px>

# Non-linear model of education
```py
import statsmodels.formula.api as smf
 
# Add a new column with educ squared
gss['educ2'] = gss['educ'] ** 2
 
# Run a regression model with educ, educ2, age, and age2
results = smf.ols('realinc ~ educ + educ2 + age + age2',data=gss).fit()
 
# Print the estimated parameters
print(results.params)
'''
Intercept   -23241.884
educ          -528.309
educ2          159.967
age           1696.717
age2           -17.197
dtype: float64
'''
```
# Making predictions
```py
# Run a regression model with educ, educ2, age, and age2
results = smf.ols('realinc ~ educ + educ2 + age + age2', data=gss).fit()

# Make the DataFrame
df = pd.DataFrame()
df['educ'] = np.linspace(0, 20)
df['age'] = 30
df['educ2'] = df['educ']**2
df['age2'] = df['age']**2

# Generate and plot the predictions
pred = results.predict(df)
print(pred.head())
'''
    0    12182.345
    1    11993.359
    2    11857.672
    3    11775.286
    4    11746.199
'''
```
# Visualizing predictions
```py
# Plot mean income in each age group
plt.clf()
grouped = gss.groupby('educ')
mean_income_by_educ = grouped['realinc'].mean()
plt.plot(mean_income_by_educ,'o',alpha=0.5)

# Plot the predictions
pred = results.predict(df)
plt.plot(df['educ'],pred, '-', label='Age 30')

# Label axes
plt.xlabel('Education (years)')
plt.ylabel('Income (1986 $)')
plt.legend()
plt.show()
```
<img src="https://user-images.githubusercontent.com/51888893/211344118-deb32c73-5dce-4653-9e0b-bef86ea57f63.png" width=400px>

# Predicting a binary variable
```py
# Recode grass
gss['grass'].replace(2, 0, inplace=True)
 
# Run logistic regression
results = smf.logit('grass ~ age + age2 + educ + educ2 + C(sex)', data=gss).fit()
results.params
'''
Intercept     -1.685e+00
C(sex)[T.2]   -3.846e-01
age           -3.476e-02
age2           1.917e-04
educ           2.219e-01
educ2         -4.163e-03
dtype: float64
'''
```
```py
# Set the education level to 12
df['educ'] = 12
df['educ2'] = df['educ']**2
```
```py
# Generate predictions for men and women
df['sex'] = 1
pred1 = results.predict(df)
 
df['sex'] = 2
pred2 = results.predict(df)
```
```py
grouped = gss.groupby('age')
favor_by_age = grouped['grass'].mean()
plt.clf()
plt.plot(favor_by_age, 'o', alpha=0.5)
 
plt.plot(df['age'], pred1, label='Male')
plt.plot(df['age'], pred2, label='Female')
 
plt.xlabel('Age')
plt.ylabel('Probability of favoring legalization')
plt.legend()
plt.show()
```
<img src="https://user-images.githubusercontent.com/51888893/211355865-cae61842-777b-46d9-92c4-f5ea9858f14e.png" width=400px>

